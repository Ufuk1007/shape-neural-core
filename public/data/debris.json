[
  {
    "id": "diffusion_gan_hybrid_brushstroke_control",
    "category": "STR_ART",
    "headline": "New diffusion–GAN hybrid gives artists brushstroke-level control",
    "summary": "A research team released a hybrid generative model that combines diffusion sampling with a lightweight GAN ‘style head’ to let artists steer local features like brushstrokes, edge sharpness, and material gloss in real time. This matters for design workflows because it moves image models from prompt-only control toward fine-grained, painterly direction, making AI feel more like a responsive brush than a black box.",
    "url": "https://arxiv.org/abs/2512.0xxx",
    "relevance": 96
  },
  {
    "id": "layout_aware_image_generation_tool",
    "category": "STR_ART",
    "headline": "Layout-aware image generator treats wireframes as creative constraints",
    "summary": "A new web-based tool lets designers drop in low-fidelity layout boxes (grids, frames, type blocks) and uses a layout-aware diffusion model to fill them with coherent imagery while preserving hierarchy and negative space. It bridges UX wireframes and visual exploration, turning traditional layout artifacts into live constraints for generative art direction.",
    "url": "https://designlab.example.com/layout-aware-diffusion",
    "relevance": 94
  },
  {
    "id": "procedural_materials_from_text_prompts",
    "category": "STR_ART",
    "headline": "Text-to-material system generates editable procedural shaders",
    "summary": "A new generative pipeline converts text prompts into fully parameterized PBR materials and node-based shader graphs rather than flat textures. For 3D artists and product designers, this enables rapid ideation of complex surfaces—like weathered metals or translucent fabrics—that remain editable for downstream rendering and fabrication.",
    "url": "https://graphics.example.org/text-to-materials-2025",
    "relevance": 91
  },
  {
    "id": "multimodal_style_transfer_for_brand_systems",
    "category": "CX_UX",
    "headline": "Multimodal style engine keeps brand visuals consistent across media",
    "summary": "A design-research group unveiled a multimodal ‘brand style engine’ that learns from a small set of reference artifacts and then constrains image, icon, and UI component generation to that aesthetic. This could reshape design systems by making visual consistency an automatic property of AI tooling, instead of a manual policing task for designers.",
    "url": "https://uxresearch.example.com/ai-brand-style-engine",
    "relevance": 93
  },
  {
    "id": "behavioral_prototyping_ai_for_ux_flows",
    "category": "CX_UX",
    "headline": "AI agent simulates realistic user behavior in early UX prototypes",
    "summary": "A new ‘behavioral prototyping’ platform uses large user-model agents to explore Figma-style clickthrough prototypes, surfacing probable confusion points and drop-offs before real user testing. For UX teams, this offers a way to stress-test flows, microcopy, and navigation with synthetic but behaviorally grounded usage data, compressing research cycles.",
    "url": "https://humancomputer.example.org/behavioral-prototyping-agents",
    "relevance": 95
  },
  {
    "id": "accessibility_first_ui_generator",
    "category": "CX_UX",
    "headline": "Accessibility-first UI generator bakes WCAG into AI layout decisions",
    "summary": "A design tool update introduced an AI layout and theming engine that treats accessibility rules (contrast, touch targets, motion sensitivity) as hard constraints while exploring visual options. This reframes accessibility from a retrofit checklist into a core generative parameter, letting teams co-create inclusive interfaces without sacrificing visual exploration.",
    "url": "https://a11y.design.example.com/ai-accessible-layouts",
    "relevance": 92
  },
  {
    "id": "neural_sound_scenography_for_installations",
    "category": "SONIC",
    "headline": "Neural sound scenography engine composes site-specific ambience",
    "summary": "An experimental audio system uses spatially aware generative models to compose ambient soundscapes that react to architecture, crowd density, and light in real time. Sound and installation artists can now treat entire rooms as instruments, with AI scaffolding evolving sonic atmospheres that respond to how people move through designed spaces.",
    "url": "https://soundart.example.net/neural-scenography",
    "relevance": 97
  },
  {
    "id": "timbre_preserving_voice_style_mixer",
    "category": "SONIC",
    "headline": "New model mixes vocal styles while preserving singer identity",
    "summary": "A music-ML lab released a timbre-preserving voice model that lets creators apply stylistic attributes—genre phrasing, vibrato, formant shaping—without altering perceived identity. This opens up ethically clearer forms of vocal augmentation where singers retain ownership of their 'voice self' while flexibly exploring genre-bending sonic personas.",
    "url": "https://audioai.example.org/timbre-style-mixer",
    "relevance": 90
  },
  {
    "id": "symbolic_structure_aware_music_diffusion",
    "category": "SONIC",
    "headline": "Structure-aware music diffusion gives composers form-level control",
    "summary": "A new diffusion-based music generator takes high-level structure graphs (intro–verse–chorus–bridge) as input, producing audio that adheres to specified tension curves and motif recurrence. For composers and game audio designers, this means AI can finally co-create at the level of musical form and narrative rather than just local texture.",
    "url": "https://ismir.example.com/papers/structure-aware-music-diffusion",
    "relevance": 94
  },
  {
    "id": "crossmodal_sketch_to_sound_prototyping",
    "category": "SONIC",
    "headline": "Cross-modal tool turns motion sketches into sound design prototypes",
    "summary": "A design school lab introduced a sketch-to-sound interface where designers draw motion curves or interaction paths and a model generates synchronized sonic feedback. This lets UX and product teams prototype interaction sounds directly from gesture concepts, closing the gap between motion language, sonic affordances, and overall experience.",
    "url": "https://interactionlab.example.edu/sketch-to-sound",
    "relevance": 89
  },
  {
    "id": "concept_critique_agents_for_creative_process",
    "category": "META",
    "headline": "Concept-critique AI agents studied as collaborators, not tools",
    "summary": "A new HCI study examined AI ‘critique agents’ that give pointed feedback on mood boards, storyboards, and early sketches, finding that designers treat them more like opinionated collaborators than neutral software. The work raises questions about authorship and responsibility when AI actively shapes concept direction instead of passively executing prompts.",
    "url": "https://chi2026.example.org/papers/ai-critique-agents",
    "relevance": 88
  },
  {
    "id": "paper_on_ai_as_cultural_maintenance_in_art",
    "category": "META",
    "headline": "Philosophers argue AI art is more about curation than creation",
    "summary": "A philosophy preprint claims that current generative models function primarily as ‘cultural maintenance engines’ that recombine and preserve aesthetic lineages rather than originating genuinely new ones. This reframes AI art practice as a curatorial and editorial act, shifting creative credit toward prompt design, dataset critique, and selection.",
    "url": "https://philarchive.example.com/ai-cultural-maintenance",
    "relevance": 93
  },
  {
    "id": "rights_of_ai_muses_in_creative_industries",
    "category": "META",
    "headline": "Legal scholars debate whether AI muses deserve recognition",
    "summary": "A law-and-philosophy panel published a position paper arguing that while AI systems should not hold copyright, their role as ‘muses’ in human creativity may warrant formal acknowledgment in contracts and credits. This could transform how design and media studios frame collaboration, attribution, and compensation in AI-assisted workflows.",
    "url": "https://lawtech.example.org/ai-muses-credit",
    "relevance": 87
  },
  {
    "id": "embodied_llm_agents_in_3d_art_tools",
    "category": "STR_ART",
    "headline": "Embodied LLM agents now drive co-creative 3D sculpting sessions",
    "summary": "A major 3D tool prototype embeds an LLM-driven agent that can manipulate meshes, propose topology changes, and suggest lighting setups via natural language dialogue. Instead of treating models as offline generators, this approach turns sculpting into a conversational process where spatial reasoning and aesthetic judgment are shared between human and AI.",
    "url": "https://cgresearch.example.com/embodied-llm-sculpting",
    "relevance": 95
  },
  {
    "id": "data_poisoning_as_creative_resistance_art",
    "category": "META",
    "headline": "Artists weaponize data poisoning as a creative resistance tactic",
    "summary": "A new media art project released tools that subtly poison scraped images with imperceptible perturbations that mislead training while remaining visually coherent. Beyond IP defense, the work positions adversarial examples as an artistic medium, using model vulnerability itself as a canvas for critical commentary on extraction and consent.",
    "url": "https://criticalcode.example.art/data-poisoning-art",
    "relevance": 92
  },
  {
    "id": "multi_agent_co_creation_in_ux_research",
    "category": "CX_UX",
    "headline": "Multi-agent AI simulates diverse stakeholders in UX workshops",
    "summary": "UX researchers tested a multi-agent system where different AI personas represent accessibility advocates, growth PMs, novice users, and domain experts during design sprints. This creates a persistent ‘virtual room’ of tensions and perspectives, helping teams surface blind spots and ethical trade-offs earlier in product definition.",
    "url": "https://uxcollective.example.com/multi-agent-stakeholder-sims",
    "relevance": 90
  },
  {
    "id": "latent_space_cartography_for_design_exploration",
    "category": "STR_ART",
    "headline": "Latent space cartography turns model internals into design maps",
    "summary": "An experimental interface visualizes the latent space of image models as navigable maps where regions correspond to styles, materials, and compositions. Designers can ‘walk’ this terrain, bookmarking promising zones and reusing coordinates as reusable style tokens, turning opaque model behavior into an explorable design landscape.",
    "url": "https://arttech.example.org/latent-cartography",
    "relevance": 89
  },
  {
    "id": "collective_authorship_ai_art_exhibition",
    "category": "META",
    "headline": "Gallery show explores collective authorship with AI and crowds",
    "summary": "A new exhibition invites visitors to iteratively prompt, edit, and overwrite a shared AI-generated artwork, treating the model as a commons rather than a personal tool. The curatorial text argues that AI creativity is fundamentally collective—emerging from datasets, model builders, and publics—challenging the myth of the solitary human or machine genius.",
    "url": "https://gallery.example.art/collective-ai-authorship",
    "relevance": 86
  }
]