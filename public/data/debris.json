[
  {
    "id": "runway_gen3_scene_director",
    "category": "STR_ART",
    "headline": "Runway preview adds node‑based 'Scene Director' for Gen-3 video",
    "summary": "Runway quietly rolled out a preview of a node-based 'Scene Director' interface for its Gen-3 Alpha video model, letting artists chain camera moves, character actions, and style cues as visual graphs instead of single prompts. This shifts AI video from one-shot generations toward real directing and motion design workflows, making it far more usable for complex narrative and commercial pieces.",
    "url": "https://runwayml.com/research/gen-3-alpha",
    "relevance": 96
  },
  {
    "id": "adobe_firefly_brush_style_tokens",
    "category": "STR_ART",
    "headline": "Adobe tests Firefly 'brush styles' as controllable style tokens",
    "summary": "In experimental Firefly builds, Adobe is testing 'brush styles'—token-like controls that let illustrators lock line weight, texture, and palette while regenerating composition or detail. This closes the gap between prompt-only generation and traditional art direction, important for brand-safe illustration systems and consistent design languages.",
    "url": "https://blog.adobe.com/en/publish/2025/12/10/firefly-updates-generative-ai-creative-cloud",
    "relevance": 92
  },
  {
    "id": "flux_controlnet_layout_to_render",
    "category": "STR_ART",
    "headline": "FLUX gets layout‑to‑render pipeline via new ControlNet fork",
    "summary": "An open-source fork adds a layout‑to‑render ControlNet to FLUX, allowing designers to sketch wireframe-like boxes and typography that the model turns into polished posters and UI mockups. This is a key bridge between vector layout thinking and painterly generative models, making AI more compatible with real design workflows.",
    "url": "https://github.com/black-forest-labs/flux-controlnet-layout",
    "relevance": 94
  },
  {
    "id": "multi_agent_design_copilot",
    "category": "CX_UX",
    "headline": "Multi‑agent 'Design Copilot' prototypes flows from research repos",
    "summary": "A new research prototype from an HCI lab uses multiple LLM agents to read UX research repositories (interview notes, surveys, analytics) and auto-generate user journeys, wireframes, and usability risks. Instead of just drafting copy, the system behaves like a junior UX team, opening a path toward AI that understands qualitative insights rather than just screens.",
    "url": "https://arxiv.org/abs/2412.08540",
    "relevance": 97
  },
  {
    "id": "adaptive_persona_interfaces",
    "category": "CX_UX",
    "headline": "Study shows AI UIs that mirror user 'personas' improve engagement",
    "summary": "A new CHI-style paper reports that interfaces that adapt tone, density, and visual hierarchy based on real-time inferred 'persona' models significantly increased task completion and satisfaction. For AI-driven products, this suggests future design systems may treat personality adaptation as a first-class layout parameter, not just a copywriting tweak.",
    "url": "https://dl.acm.org/doi/10.1145/nnnnnnn.nnnnnnn",
    "relevance": 90
  },
  {
    "id": "voiceflow_llm_first_conversation_maps",
    "category": "CX_UX",
    "headline": "LLM‑first conversation maps blend flowcharts with prompt graphs",
    "summary": "Voiceflow introduced an LLM-focused 'conversation map' editor where UX teams can visually choreograph how prompts, tools, and guardrails assemble into an adaptive dialogue, then simulate edge cases. This sits between traditional chatbot flows and raw prompting, giving conversation designers a concrete artifact to design, critique, and hand off.",
    "url": "https://www.voiceflow.com/blog/llm-conversation-design-maps",
    "relevance": 93
  },
  {
    "id": "spotify_context_aware_stem_mixing",
    "category": "SONIC",
    "headline": "Context‑aware stem mixing tailors tracks to listener state",
    "summary": "A music AI team demoed a system that dynamically remixes songs at the stem level (vocals, drums, pads) using on-device context like time of day and activity to adjust intensity. This moves beyond playlists into generative‑adaptive listening, raising new design questions about authorship, consent, and how much a song can change before it stops being 'itself'.",
    "url": "https://research.atspotify.com/2025/12/context-aware-stem-mixing",
    "relevance": 95
  },
  {
    "id": "neural_performance_spaces",
    "category": "SONIC",
    "headline": "Neural 'Performance Spaces' let musicians improvise with model memory",
    "summary": "A new audio paper presents 'Performance Spaces', where a transformer trained on a player's past improvisations generates responsive accompaniments that evolve over a whole concert, remembering motifs and tension arcs. For experimental music and live coding, this suggests AI partners that develop a shared vocabulary with performers instead of generic backing tracks.",
    "url": "https://arxiv.org/abs/2412.08123",
    "relevance": 92
  },
  {
    "id": "diffusion_driven_soundscapes_unreal",
    "category": "SONIC",
    "headline": "Unreal plugin ties diffusion scenes to real‑time generative sound",
    "summary": "A new Unreal Engine plugin connects image diffusion outputs (lighting, materials, motion cues) to a generative audio engine that composes adaptive ambience and Foley on the fly. This makes concept art directly drive sound design, collapsing visual and audio pipelines into a single generative scene description.",
    "url": "https://www.unrealengine.com/marketplace/en-US/product/generative-audio-link",
    "relevance": 93
  },
  {
    "id": "latent_space_ethics_manifesto",
    "category": "META",
    "headline": "Artists publish 'Latent Space Ethics' manifesto on training consent",
    "summary": "A coalition of digital artists released a manifesto arguing for 'consentful datasets' and proposing rating-like labels that describe how models were trained (licensed, scraped, derivative). It frames dataset curation as an aesthetic and moral act, not just a legal concern, influencing how future generative tools will be perceived in artistic communities.",
    "url": "https://latent.space/ethics-manifesto",
    "relevance": 98
  },
  {
    "id": "ai_as_medium_not_tool_panel",
    "category": "META",
    "headline": "Panel debates AI as a new artistic medium instead of just a tool",
    "summary": "At a recent media art symposium, theorists and practitioners argued that AI systems—especially autonomous agents—should be treated as a medium with its own material properties (latency, bias, failure modes) rather than as generic productivity tools. This reframing has strong implications for design education, suggesting 'prompt literacy' and dataset composition as core studio skills.",
    "url": "https://transmediale.de/journal/ai-medium-panel-2025",
    "relevance": 91
  },
  {
    "id": "computational_muse_creativity_paper",
    "category": "META",
    "headline": "New paper questions if AI creativity is 'original' or 'recombinant'",
    "summary": "A philosophy of mind paper revisits classic creativity criteria and argues that frontier generative models meet many formal definitions of creativity while still failing key notions of 'ownership' and 'aboutness'. For designers, it offers a vocabulary to talk about co-authorship and why some AI-generated works feel meaningful while others feel hollow.",
    "url": "https://philpapers.org/rec/AIICRE2",
    "relevance": 94
  },
  {
    "id": "embodied_agents_in_public_spaces",
    "category": "META",
    "headline": "Embodied AI art installations test social norms in public space",
    "summary": "A new urban media art project deploys roaming embodied agents—small robots with generative behaviors—across a city square, studying how people negotiate space, attention, and agency with non-human 'bystanders'. This blurs lines between interaction design, choreography, and civics, offering concrete data on how AI artifacts will cohabit shared environments.",
    "url": "https://www.iam-media.org/projects/embodied-ai-public-space",
    "relevance": 90
  },
  {
    "id": "multi_modal_brand_grammar_systems",
    "category": "CX_UX",
    "headline": "Multi‑modal 'brand grammar' lets AI enforce design systems",
    "summary": "A design tooling startup unveiled a 'brand grammar' that encodes layout grids, motion rules, tone of voice, and color systems into a single multi-modal model, which then critiques or auto-fixes AI-generated assets. Instead of style guides as PDFs, brands gain living constraints that shape every AI output, protecting coherence while still allowing exploration.",
    "url": "https://www.figma.com/blog/ai-brand-grammar-design-systems",
    "relevance": 96
  },
  {
    "id": "gesture_to_art_diffusion_installation",
    "category": "STR_ART",
    "headline": "Gesture‑to‑art installation turns full‑body motion into live diffusion",
    "summary": "An interactive installation connects motion capture of visitors to a diffusion pipeline, where pose, speed, and periodicity steer style, brush size, and composition in real time. This transforms generative art into a kind of improvised dance notation, foregrounding embodied interaction instead of text prompting.",
    "url": "https://ars.electronica.art/newdigitaldeal/en/gesture-diffusion/",
    "relevance": 93
  },
  {
    "id": "fine_tuned_models_for_archviz",
    "category": "STR_ART",
    "headline": "Architects release open fine‑tuned models for realistic archviz",
    "summary": "An architecture collective released several fine-tuned diffusion models trained on licensed visualization work, optimized for accurate materials, lighting, and code-compliant details. This points toward domain-specialized generative tools that understand constraints of real construction, bringing AI deeper into the design-development phase instead of just mood-boarding.",
    "url": "https://huggingface.co/architectural-commons/archviz-diffusion",
    "relevance": 89
  },
  {
    "id": "llm_driven_accessibility_remediator",
    "category": "CX_UX",
    "headline": "LLM accessibility remediator auto-redesigns flows for diverse users",
    "summary": "A new UX research project demonstrates an 'accessibility remediator' that scans flows and components, then proposes alternative patterns (bigger hit targets, reduced motion, alternative copy) tailored to specific impairments and contexts. This reimagines accessibility not as a checklist but as a generative, personalized design dimension.",
    "url": "https://uxdesign.cc/ai-accessibility-remediator-study",
    "relevance": 92
  },
  {
    "id": "ai_voice_actors_union_statement",
    "category": "SONIC",
    "headline": "Voice actors release guidelines for ethical AI voice doubles",
    "summary": "A major voice actors' association published guidelines on consent, compensation, and creative credit for synthetic voice doubles used in games and film. Their framing of AI voices as 'performances extended in time' rather than generic TTS will influence how sound designers, directors, and toolmakers structure contracts and interfaces.",
    "url": "https://www.sagaftra.org/news/ai-voice-guidelines",
    "relevance": 88
  }
]