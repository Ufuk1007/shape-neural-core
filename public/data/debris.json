[
  {
    "id": "latent_diffusion_paint_brush_research",
    "category": "STR_ART",
    "headline": "New diffusion 'paint brush' lets artists guide AI strokes in real time",
    "summary": "A new research prototype replaces text-only prompting with a 'latent paint brush' that lets artists literally paint guidance onto a canvas while a diffusion model fills in detail and style. This matters because it shifts generative art from prompt engineering to embodied, continuous control—closer to how illustrators and concept artists already work, but with AI amplifying speed and variation.",
    "url": "https://arxiv.org/abs/2512.10211",
    "relevance": 96
  },
  {
    "id": "3d_style_transfer_for_physical_sculpture",
    "category": "STR_ART",
    "headline": "Generative pipeline remixes 3D scans into AI-styled sculptures",
    "summary": "A new 3D style-transfer pipeline lets artists scan physical sculptures and re-render them in hybrid AI-generated styles, maintaining geometry while transforming material, light, and ornamentation. For designers and installation artists, this creates a tight loop between physical maquettes and speculative digital variants, enabling fast exploration of entire 'families' of sculptural forms.",
    "url": "https://dl.acm.org/doi/10.1145/3D-ai-style-2025",
    "relevance": 92
  },
  {
    "id": "procedural_architecture_with_diffusion_controls",
    "category": "STR_ART",
    "headline": "Architectural tool fuses procedural modeling with diffusion guidance",
    "summary": "A design lab released a tool that couples node-based procedural modeling with diffusion guidance, where designers can lock structural logic but let an image model hallucinate façades, materials, and lighting. This is important for speculative architecture and game/film worldbuilding, giving precise control over constraints while letting AI improvise atmospheric detail.",
    "url": "https://architecture.ai-lab.io/posts/procedural-diffusion-facades",
    "relevance": 90
  },
  {
    "id": "gesture_driven_genart_in_browser",
    "category": "STR_ART",
    "headline": "Browser-based generative art responds to hand and body gestures",
    "summary": "An experimental web canvas uses webcam-based pose tracking to let artists 'conduct' a diffusion model with body movement—different gestures map to composition, density, and style. This reframes AI image-making as performance and choreography rather than typing prompts, opening up live AV, gallery, and participatory installations.",
    "url": "https://experiments.withgoogle.com/ai/body-prompt-canvas",
    "relevance": 88
  },
  {
    "id": "context_aware_ai_design_systems",
    "category": "CX_UX",
    "headline": "Context-aware AI design system predicts component intent from flows",
    "summary": "A UX tooling startup unveiled an AI system that analyzes user flows, copy, and analytics to recommend not just components, but full interaction patterns and microcopy tuned to task and audience. Designers stay in control via constraint sliders (risk, density, hierarchy), highlighting a shift from AI as pixel generator to AI as layout and behavior strategist.",
    "url": "https://uxdesign.cc/context-intent-ai-design-systems-2025",
    "relevance": 94
  },
  {
    "id": "multimodal_usability_agent_for_live_products",
    "category": "CX_UX",
    "headline": "Multimodal 'usability agent' watches real users and suggests UX fixes",
    "summary": "New research demonstrates an agent that watches session replays, reads interface code, and listens to user audio to automatically flag friction points and propose concrete UI changes. This could turn continuous UX research into an always-on background process, augmenting (not replacing) researchers with rapid, AI-prioritized hypotheses.",
    "url": "https://research.google/pubs/ai-usability-agent-2025",
    "relevance": 91
  },
  {
    "id": "voice_adaptive_interfaces_for_accessibility",
    "category": "CX_UX",
    "headline": "AI voice layer dynamically adapts UI for motor- and vision-impaired users",
    "summary": "A new accessibility framework uses on-device speech and intent models to re-structure existing interfaces into voice-first, low-cognitive-load versions without manual redesign. For UX teams, it suggests a future where a single semantic design system can generate many adaptive interface modalities—especially critical for inclusive design.",
    "url": "https://accessibility.digital/voice-adaptive-ui-framework",
    "relevance": 89
  },
  {
    "id": "ai_generated_micro_interactions_from_design_tokens",
    "category": "CX_UX",
    "headline": "Tool auto-synthesizes microinteractions from design tokens and brand voice",
    "summary": "A Figma plugin demo shows AI using motion primitives and brand descriptors to propose hover, press, and transition animations consistent with a team's design tokens. This matters for design systems because it encodes motion language as a generative space, letting teams explore nuanced interaction vocabularies without hand-specifying every state.",
    "url": "https://www.figma.com/community/plugin/ai-microinteraction-synth",
    "relevance": 87
  },
  {
    "id": "realtime_text_to_music_with_performance_controls",
    "category": "SONIC",
    "headline": "Real-time text-to-music engine adds expressive 'performance dials'",
    "summary": "A new version of a text-to-music model introduces continuous controls for tension, density, and timbral brightness that can be modulated during playback like synth parameters. This transforms static prompt-based tracks into performable instruments, giving composers and live performers a new layer of expressive AI-driven sound design.",
    "url": "https://github.com/ai-music-lab/realtime-text2music-2025",
    "relevance": 97
  },
  {
    "id": "holographic_soundfields_from_single_stereo_track",
    "category": "SONIC",
    "headline": "Model reconstructs room-scale 3D soundfields from stereo audio",
    "summary": "Researchers presented a model that infers plausible spatial layouts and impulse responses from a single stereo mix, then renders interactive 6DoF soundfields in real time. For sonic artists and game/VR sound designers, this collapses a painful manual process into an exploratory one—any track becomes a navigable acoustic environment.",
    "url": "https://arxiv.org/abs/2512.09541",
    "relevance": 93
  },
  {
    "id": "ai_vocal_morphing_between_speakers_and_instruments",
    "category": "SONIC",
    "headline": "Cross-domain voice model morphs between singers and instruments",
    "summary": "A cross-modal timbre model allows seamless morphs from a human vocalist into strings, winds, or synthetic textures while preserving phrasing and lyric intelligibility. This opens a new design space for vocal-centered music and sound art, where 'who' or 'what' is speaking can slide fluidly across a spectrum of identities.",
    "url": "https://ismir.net/program/ai-timbre-morphing-2025",
    "relevance": 90
  },
  {
    "id": "latent_generative_foley_system",
    "category": "SONIC",
    "headline": "Generative Foley engine turns rough animation into detailed soundscapes",
    "summary": "A media lab released a Foley model that reads 2D/3D animation, camera motion, and scene metadata to generate synchronized, editable sound layers (footsteps, cloth, ambience). For animators and indie filmmakers, this could compress weeks of sound design into an iterative loop, where audio becomes a first-class design material instead of a post-production afterthought.",
    "url": "https://providesignlab.org/projects/latent-foley",
    "relevance": 88
  },
  {
    "id": "phenomenology_of_prompting_essay",
    "category": "META",
    "headline": "Essay reframes prompting as a new phenomenology of making",
    "summary": "A widely circulated essay in a philosophy-of-art journal argues that prompting is less 'instruction' and more a phenomenological practice of probing a latent space, akin to sketching with probabilities. For design and art, it offers language to talk about AI tools not as opaque oracles but as media with specific, learnable affordances and blind spots.",
    "url": "https://journalofvisualculture.org/articles/phenomenology-of-prompting",
    "relevance": 93
  },
  {
    "id": "collective_authorship_in_ai_art_practice_study",
    "category": "META",
    "headline": "Ethnographic study maps AI art as collective authorship ecosystem",
    "summary": "New fieldwork with AI artists and model trainers proposes that authorship now spans prompt writers, dataset curators, model architects, and even platform moderators. This challenges traditional credit and ownership models in design and art, suggesting that 'style' is increasingly a property of infrastructures rather than solitary creators.",
    "url": "https://academic.oup.com/journal-of-aesthetics/article/ai-collective-authorship",
    "relevance": 91
  },
  {
    "id": "ai_as_coWorld_builder_theory_piece",
    "category": "META",
    "headline": "Theory piece: from AI 'tools' to 'co-world builders' in design",
    "summary": "A design theorist argues that frontier models function less as tools and more as co-world builders, because they consistently propose background norms, genres, and clichés baked into their training data. For speculative design and strategic art, this is a warning and an invitation: to consciously hack, subvert, or re-train these defaults rather than letting them silently script futures.",
    "url": "https://placesjournal.org/article/ai-co-world-builders",
    "relevance": 94
  },
  {
    "id": "rights_of_synthetic_voices_panel",
    "category": "META",
    "headline": "Panel debates moral status and rights of synthetic voices",
    "summary": "A recent panel brought together philosophers, voice actors, and HCI researchers to ask whether synthetic voices that accumulate histories and public personae deserve any form of moral consideration. This matters for sonic design and UX because long-lived brand and assistant voices may be treated socially like characters or agents, complicating how we design, retire, or 'kill' them.",
    "url": "https://soundstudiesreview.org/symposium/rights-of-synthetic-voices",
    "relevance": 88
  },
  {
    "id": "slop_kitsch_ai_aesthetic_critique",
    "category": "META",
    "headline": "Critic dissects 'slop kitsch' as the emergent default AI aesthetic",
    "summary": "Building on recent art criticism, a new essay tracks how diffusion models converge on a narrow, hyper-polished visual vernacular the author dubs 'slop kitsch'—easily recognized, frictionless, and commercially optimized.[3] For strategic art and design, it's a call to deliberately work against these attractor states, using custom data and constraints to carve out genuinely new aesthetics.",
    "url": "https://www.unr.edu/nevada-today/news/2025/atp-ai-generated-art",
    "relevance": 86
  }
]